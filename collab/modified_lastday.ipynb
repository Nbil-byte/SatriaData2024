{"cells":[{"cell_type":"code","execution_count":1,"id":"27aaafb9-440f-4124-8fd8-81d8a64c27f1","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"27aaafb9-440f-4124-8fd8-81d8a64c27f1","executionInfo":{"status":"ok","timestamp":1718787317844,"user_tz":-420,"elapsed":24758,"user":{"displayName":"Nabil Hamzah Ash-Shiddiq","userId":"09756051695197131721"}},"outputId":"7ab26e69-55f4-4ef6-fb58-2b3e44b55e5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/SatriaData2024\n"]}],"source":["from google.colab import drive\n","import os\n","\n","# Mount Google Drive ke direktori default\n","drive.mount('/content/drive')\n","\n","# Setel direktori kerja ke subdirektori yang diinginkan\n","subdir = '/content/drive/MyDrive/SatriaData2024'\n","os.makedirs(subdir, exist_ok=True)\n","os.chdir('/content/drive/MyDrive/SatriaData2024')\n","\n","# Verifikasi direktori kerja saat ini\n","!pwd\n"]},{"cell_type":"code","execution_count":null,"id":"02daae5a-b623-481d-a852-124fd85b1083","metadata":{"tags":[],"id":"02daae5a-b623-481d-a852-124fd85b1083","outputId":"d3175872-1aef-4f47-b757-5e6047fd1085"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-06-19 04:13:24.733427: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2024-06-19 04:13:24.775066: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-06-19 04:13:25.697559: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["import pandas as pd\n","from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n","from datasets import Dataset\n","import torch\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from imblearn.over_sampling import SMOTE\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from nltk.corpus import stopwords\n","import numpy as np\n","from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score, f1_score\n"]},{"cell_type":"code","execution_count":null,"id":"dd8280bb-8853-4ac9-ad78-12dc2fac5f1e","metadata":{"tags":[],"id":"dd8280bb-8853-4ac9-ad78-12dc2fac5f1e","outputId":"04b70c09-d6b9-4f6e-c56d-2cbba2527d33"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Kunjungan Prabowo ini untuk meresmikan dan men...</td>\n","      <td>Sumber Daya Alam</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>RT Anies dapat tepuk tangan meriah saat jadi R...</td>\n","      <td>Politik</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>@CIqXqwGAT04tMtx4OCATxjoVq7vv/Y8HeYaIOgMFg8Y= ...</td>\n","      <td>Demografi</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>RT @L3R8XFBw3WGbxRPSj0/0hHZTbqVGX7qtfwRg9zmhK7...</td>\n","      <td>Politik</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Anies Baswedan Harap ASN termasuk TNI dan Polr...</td>\n","      <td>Politik</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text             label\n","0  Kunjungan Prabowo ini untuk meresmikan dan men...  Sumber Daya Alam\n","1  RT Anies dapat tepuk tangan meriah saat jadi R...           Politik\n","2  @CIqXqwGAT04tMtx4OCATxjoVq7vv/Y8HeYaIOgMFg8Y= ...         Demografi\n","3  RT @L3R8XFBw3WGbxRPSj0/0hHZTbqVGX7qtfwRg9zmhK7...           Politik\n","4  Anies Baswedan Harap ASN termasuk TNI dan Polr...           Politik"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["data = pd.read_csv('data/dataset_penyisihan_bdc_2024.csv')\n","data.head()"]},{"cell_type":"code","execution_count":null,"id":"8ed0312c-f291-42e3-b57f-ab424d5d324f","metadata":{"tags":[],"colab":{"referenced_widgets":["a72bff9d56144f848ededb774797dac5"]},"id":"8ed0312c-f291-42e3-b57f-ab424d5d324f","outputId":"45854db1-40ad-4939-f4d4-546b1507676f"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a72bff9d56144f848ededb774797dac5","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['text', 'label'],\n","    num_rows: 5000\n","})\n"]}],"source":["dataset = Dataset.from_pandas(data)\n","\n","# Kamus untuk memetakan label teks ke integer\n","label_to_id = {\n","    \"Sumber Daya Alam\": 0,\n","    \"Politik\": 1,\n","    \"Demografi\": 2,\n","    \"Pertahanan dan Keamanan\": 3,\n","    \"Ideologi\": 4,\n","    \"Ekonomi\": 5,\n","    \"Sosial Budaya\": 6,\n","    \"Geografi\": 7\n","}\n","\n","# Ubah label di dataset menjadi integer\n","def map_labels(example):\n","    example['label'] = label_to_id[example['label']]\n","    return example\n","\n","# Terapkan fungsi map_labels ke dataset\n","dataset = dataset.map(map_labels)\n","\n","# Verifikasi bahwa label telah diubah menjadi integer\n","print(dataset)"]},{"cell_type":"code","execution_count":null,"id":"50d621f3-825c-4ff6-a90d-c9db6815ec34","metadata":{"tags":[],"id":"50d621f3-825c-4ff6-a90d-c9db6815ec34"},"outputs":[],"source":["# Split data training dan testing menggunakan stratify (karena data imbalance)\n","train_data, test_data = train_test_split(dataset, test_size=0.2, stratify=dataset['label'], random_state=42)\n","\n","train_data = Dataset.from_dict(train_data)\n","test_data = Dataset.from_dict(test_data)\n"]},{"cell_type":"code","execution_count":null,"id":"df1d78af-1011-4b55-9aea-cbe625f626d3","metadata":{"tags":[],"id":"df1d78af-1011-4b55-9aea-cbe625f626d3","outputId":"8c3acbc7-2f0a-4183-aad2-172b233bd461"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["   label  count\n","0      1   2378\n","1      6    470\n","2      2    392\n","3      3    320\n","4      4    320\n","5      0    306\n","6      5    294\n","7      7    256\n"]}],"source":["import nltk\n","import random\n","from nltk.corpus import wordnet\n","from nltk.corpus import stopwords\n","from collections import Counter\n","from datasets import Dataset\n","\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","stop_words = stopwords.words('indonesian')\n","\n","def synonym_replacement(words, stop_words):\n","    new_words = []\n","    for word in words:\n","        if word not in stop_words:\n","            synonyms = wordnet.synsets(word)\n","            if synonyms:\n","                synonym = random.choice(synonyms[0].lemma_names()).lower()\n","                new_words.append(synonym)\n","            else:\n","                new_words.append(word)\n","        else:\n","            new_words.append(word)\n","    return new_words\n","\n","def random_insertion(words, stop_words):\n","    new_words = words.copy()\n","    synonyms = random.sample(stop_words, random.randint(1, len(stop_words) // 5))\n","    for synonym in synonyms:\n","        random_index = random.randint(0, len(new_words))\n","        new_words.insert(random_index, synonym)\n","    return new_words\n","\n","def random_deletion(words, stop_words):\n","    new_words = words.copy()\n","    if len(new_words) > 2:  # Avoid deleting everything\n","        deletion_index = random.randint(0, len(new_words) - 1)\n","        if new_words[deletion_index] not in stop_words:\n","            del new_words[deletion_index]\n","    return new_words\n","\n","def augment_data(example):\n","    augmented_texts = []\n","    for text in example['text']:\n","        words = text.split()\n","        augmented_text = ' '.join(synonym_replacement(words, stop_words) + random_insertion(words.copy(), stop_words) + random_deletion(words.copy(), stop_words))\n","        augmented_texts.append(augmented_text)\n","    return {'text': augmented_texts, 'label': example['label']}\n","\n","def balance_data(train_data, min_count=200):\n","    label_counter = Counter(train_data['label'])\n","    data_by_label = {label: [] for label in label_counter}\n","\n","    for text, label in zip(train_data['text'], train_data['label']):\n","        data_by_label[label].append(text)\n","\n","    augmented_train_data = {'text': [], 'label': []}\n","\n","    for label, texts in data_by_label.items():\n","        while len(data_by_label[label]) < min_count:\n","            example = {'text': texts, 'label': [label] * len(texts)}\n","            augmented_example = augment_data(example)\n","            data_by_label[label].extend(augmented_example['text'])\n","        augmented_train_data['text'].extend(data_by_label[label])\n","        augmented_train_data['label'].extend([label] * len(data_by_label[label]))\n","\n","    return Dataset.from_dict(augmented_train_data)\n","\n","# Apply the balance_data function to your dataset\n","augmented_train_data = balance_data(train_data)\n","\n","# Convert the augmented data back to a DataFrame\n","augmented_df = augmented_train_data.to_pandas()\n","\n","# Calculate and display the label distribution\n","label_distribution = augmented_df['label'].value_counts().reset_index()\n","label_distribution.columns = ['label', 'count']\n","print(label_distribution)\n"]},{"cell_type":"code","execution_count":null,"id":"003670fd-2b3a-4512-9ad9-b3cf89d21ecd","metadata":{"tags":[],"id":"003670fd-2b3a-4512-9ad9-b3cf89d21ecd","outputId":"046c3505-d09c-49a2-e4ce-04804881771b"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([10.0806,  1.7030, 31.2500,  1.5625,  1.5625,  0.2103,  1.0647,  3.2552])\n"]}],"source":["# Class Weighting\n","class_weights = compute_class_weight(\n","    class_weight='balanced',\n","    classes=np.unique(data['label']),\n","    y=data['label']\n",")\n","\n","class_weights = torch.tensor(class_weights, dtype=torch.float)\n","print(class_weights)\n"]},{"cell_type":"code","execution_count":null,"id":"2d014d32-8666-410e-aaa4-e36437329799","metadata":{"tags":[],"id":"2d014d32-8666-410e-aaa4-e36437329799","outputId":"81589af1-866b-4db4-bcf0-3ee7185d721f"},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: fineGrained).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["!huggingface-cli login --token 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"]},{"cell_type":"code","execution_count":null,"id":"10866633-57da-4811-8ff1-af525d7ec797","metadata":{"tags":[],"colab":{"referenced_widgets":["d63fedd15705401892bf6e07c3c328b4","509f2a22ba89492a8e09c0268f8b2b37"]},"id":"10866633-57da-4811-8ff1-af525d7ec797","outputId":"7eaeec52-aa86-410e-e80e-f17ea1f3e6b6"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d63fedd15705401892bf6e07c3c328b4","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/4736 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"509f2a22ba89492a8e09c0268f8b2b37","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n","    num_rows: 4736\n","})\n","Dataset({\n","    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n","    num_rows: 1000\n","})\n"]}],"source":["# Import libraries\n","from datasets import Dataset as HFDataset\n","from transformers import BertTokenizer\n","\n","# Pastikan augmented_train_data dan test_data adalah objek datasets.Dataset\n","\n","# Tokenizer untuk model BERT\n","tokenizer = BertTokenizer.from_pretrained(\"indolem/indobertweet-base-uncased\")\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n","\n","# Tokenisasi dataset\n","tokenized_train_dataset = augmented_train_data.map(tokenize_function, batched=True)\n","tokenized_test_dataset = test_data.map(tokenize_function, batched=True)\n","\n","print(tokenized_train_dataset)\n","print(tokenized_test_dataset)\n"]},{"cell_type":"code","execution_count":null,"id":"782efd3b-8049-44c1-b6c0-20fdd2875871","metadata":{"tags":[],"id":"782efd3b-8049-44c1-b6c0-20fdd2875871"},"outputs":[],"source":["class WeightedTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.get(\"labels\").to(model.device)\n","        outputs = model(**inputs)\n","        logits = outputs.get(\"logits\")\n","        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights.to(model.device))\n","        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n","        return (loss, outputs) if return_outputs else loss"]},{"cell_type":"code","execution_count":null,"id":"e6d557b7-4502-4578-a932-6b1d756314aa","metadata":{"tags":[],"id":"e6d557b7-4502-4578-a932-6b1d756314aa"},"outputs":[],"source":["# Inisialisasi training argument\n","from transformers import TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=10,\n",")\n"]},{"cell_type":"code","execution_count":null,"id":"a227f1ff-eae7-4d2c-9e53-6f48df6f07b3","metadata":{"tags":[],"id":"a227f1ff-eae7-4d2c-9e53-6f48df6f07b3"},"outputs":[],"source":["from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score\n","import numpy as np\n","\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    predictions = np.argmax(pred.predictions, axis=1)\n","    return {\n","        'balanced_accuracy': balanced_accuracy_score(labels, predictions),\n","        'precision': precision_score(labels, predictions, average='weighted'),\n","        'recall': recall_score(labels, predictions, average='weighted'),\n","        'f1': f1_score(labels, predictions, average='weighted')\n","    }"]},{"cell_type":"code","execution_count":null,"id":"2e3edd00-9fe6-418c-a4a2-5297961c1116","metadata":{"tags":[],"id":"2e3edd00-9fe6-418c-a4a2-5297961c1116","outputId":"23459f40-459e-4d15-cc72-09fe9434b5f2"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indolem/indobertweet-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[42], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertForSequenceClassification\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindolem/indobertweet-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(class_weights))\n\u001b[0;32m----> 5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mWeightedTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_train_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_test_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Train data\u001b[39;00m\n\u001b[1;32m     14\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:397\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[0;32m--> 397\u001b[0m enable_full_determinism(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mseed) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mfull_determinism \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_utils.py:99\u001b[0m, in \u001b[0;36mset_seed\u001b[0;34m(seed, deterministic)\u001b[0m\n\u001b[1;32m     97\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m---> 99\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# ^^ safe to call this function even if cuda is not available\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     set_eval_frame(prior)\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_dynamo/external_utils.py:36\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/random.py:45\u001b[0m, in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n\u001b[0;32m---> 45\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/random.py:126\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    123\u001b[0m         default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[1;32m    124\u001b[0m         default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m--> 126\u001b[0m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:223\u001b[0m, in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lazy_call\u001b[39m(callable, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 223\u001b[0m         \u001b[43mcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/random.py:124\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[1;32m    123\u001b[0m     default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[0;32m--> 124\u001b[0m     \u001b[43mdefault_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}],"source":["from transformers import BertForSequenceClassification\n","\n","model = BertForSequenceClassification.from_pretrained(\"indolem/indobertweet-base-uncased\", num_labels=len(class_weights))\n","\n","trainer = WeightedTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train_dataset,\n","    eval_dataset=tokenized_test_dataset,\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Train data\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"id":"0f66c0be-7c2a-4943-b4f5-daaa07d0c588","metadata":{"tags":[],"id":"0f66c0be-7c2a-4943-b4f5-daaa07d0c588","outputId":"448d0fd4-4e51-476c-ed4e-333f4bf84519"},"outputs":[{"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[43], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","t = torch.tensor([1,2], device=device)"]},{"cell_type":"code","execution_count":null,"id":"6c445996-f00e-40f3-867e-8c00a0cedae0","metadata":{"id":"6c445996-f00e-40f3-867e-8c00a0cedae0"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"3e2711e9","metadata":{"id":"3e2711e9"},"outputs":[],"source":["\n","import pandas as pd\n","from transformers import AutoTokenizer, AutoModel\n","import torch\n","\n","# Load your data\n","# data = pd.read_csv('path_to_your_data.csv')  # Uncomment and set your data path\n","\n","# Initialize tokenizer and model from BERTweet\n","tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base')\n","model = AutoModel.from_pretrained('vinai/bertweet-base')\n","\n","# Function to clean Twitter text\n","def clean_text(text):\n","    text = text.replace('URL', '')  # remove URLs\n","    text = text.replace('USER', '') # remove user mentions\n","    return text\n","\n","# Tokenize and extract features\n","def extract_features(text):\n","    cleaned_text = clean_text(text)\n","    encoded_input = tokenizer(cleaned_text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n","    with torch.no_grad():\n","        features = model(**encoded_input)\n","    return features.last_hidden_state[:,0,:].squeeze().numpy()\n","\n","# Apply the function on the dataset\n","# data['features'] = data['text'].apply(extract_features)  # Uncomment and adapt field names as needed\n","# data.head()  # To show some of the output\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}